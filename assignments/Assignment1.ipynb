{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.\n",
    "\n",
    "## Formalia:\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/suneman/socialdataanalysis2017/wiki/Assignments) carefully before proceeding. This page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "_If you fail to follow these simple instructions, it will negatively impact your grade!_\n",
    "\n",
    "**Due date and time**: The assignment is due on Monday Februrary 27th, 2017 at 23:55. Hand in your IPython notebook file (with extension `.ipynb`) via [`http://peergrade.io`](http://peergrade.io/).\n",
    "\n",
    "**Peergrading date and time**: _Remember that after handing in you have 24 hours to evaluate a few assignments written by other members of the class_. Thus, the peer evaluations are due on Monday March 6th, 2017 at 23:55."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1A: Anscombe's quartet\n",
    "\n",
    "Full details, see Week 2's exercises.\n",
    "\n",
    "Start by downloading these four datasets: [Data 1](https://raw.githubusercontent.com/suneman/socialdataanalysis2017/master/files/data1.tsv), [Data 2](https://raw.githubusercontent.com/suneman/socialdataanalysis2017/master/files/data2.tsv), [Data 3](https://raw.githubusercontent.com/suneman/socialdataanalysis2017/master/files/data3.tsv), and [Data 4](https://raw.githubusercontent.com/suneman/socialdataanalysis2017/master/files/data4.tsv). The format is `.tsv`, which stands for _tab separated values_. \n",
    "Each file has two columns (separated using the tab character). The first column is $x$-values, and the second column is $y$-values.  \n",
    "\n",
    "It's ok to just download these files to disk by right-clicking on each one, but if you use Python and _urllib_ or _urllib2_ to get them, I'll really be impressed. If you don't know how to do that, I recommend opening up Google and typing \"download file using Python\" or something like that. When interpreting the search results remember that _stackoverflow_ is your friend.\n",
    "\n",
    "* Using the `numpy` function `mean`, calculate the mean of both $x$-values and $y$-values for each dataset. \n",
    "* Use python string formatting to print precisely two decimal places of these results to the output cell. Check out [this _stackoverflow_ page](http://stackoverflow.com/questions/8885663/how-to-format-a-floating-number-to-fixed-width-in-python) for help with the string formatting. \n",
    "* Now calculate the variance for all of the various sets of $x$- and $y$-values (to three decimal places).\n",
    "* Use `numpy` to calculate the [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) between $x$- and $y$-values for all four data sets (also to three decimal places).\n",
    "* The next step is use _linear regression_ to fit a straight line $f(x) = a x + b$ through each dataset and report $a$ and $b$ (to two decimal places). An easy way to fit a straight line in Python is using `scipy`'s `linregress`. It works like this\n",
    "```\n",
    "from scipy import stats\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "```\n",
    "* Finally, it's time to plot the four datasets using `matplotlib.pyplot`. Use a two-by-two [`subplot`](http://matplotlib.org/examples/pylab_examples/subplot_demo.html) to put all of the plots nicely in a grid and use the same $x$ and $y$ range for all four plots. And include the linear fit in all four plots. (To get a sense of what I think the plot should look like, you can take a look at my version [here](https://raw.githubusercontent.com/suneman/socialdataanalysis2017/master/files/anscombe.png).)\n",
    "* Explain - in your own words - what you think my point with this exercise is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1B: KNN\n",
    "\n",
    "For full details, see Week 4's exercises.\n",
    "\n",
    "The goal of this exercise is to create a useful real-world version of the example on pp153 in DSFS. We know from Week 3's exercises that the focus crimes `PROSTITUTION`, `DRUG/NARCOTIC` and `DRIVING UNDER THE INFLUENCE` tend to be concentrated in certain neighborhoods, so we focus on those crime types since they will make the most sense a KNN - map. \n",
    "\n",
    "* Begin by using `geoplotlib` to plot all incidents of the three crime types on their own map using [`geoplotlib.kde()`](https://github.com/andrea-cuttone/geoplotlib/blob/master/examples/kde.py). This will give you an idea of how the varioius crimes are distributed across the city.\n",
    "* Next, it's time to set up your model based on the actual data. You can use the code supplied in the book or try out `scikit-learn`'s [`KNeighborsClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). If you end up using the latter (recommended), you may want to check out [this example](http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/tutorial/plot_knn_iris.html) to get a sense of the usage.\n",
    "  - You don't have to think a lot about testing/trainig and accuracy for this exercise. We're mostly interested in creating a map that's not too problematic. **But** do calculate the number of observations of each crime-type respectively. You'll find that the levels of each crime varies (lots of drug arrests, an intermediate amount of prostitiution registered, and very little drunk driving in the dataset). Since the algorithm classifies each point according to it's neighbors, what could a consequence of this imbalance in the number of examples from each class mean for your map?\n",
    "  - You can make the dataset 'balanced' by grabbing an equal number of examples from each crime category. How do you expect that will change the KNN result? In which situations is the balanced map useful - and when is the map that data in proportion to occurrences useful? Choose which map you will work on in the following. \n",
    "* Now create an approximately square grid of point that runs over SF. You get to decide the grid-size, but I recommend somewhere between $50 \\times 50$ and $100 \\times 100$ points. I recommend plotting using `geoplotlib.dot()`. To plot in three distinct the colors, I simply ran the command three times, once for each color.\n",
    "* Visualize your model by coloring the grid, coloring each grid point according to it's category. Create a plot of this kind for models where each point is colored according to the majority of its 5, 10, and 30 nearest neighbors. Describe what happens to the map as you increase the number of neighbors, `K`.  \n",
    "\n",
    "**NOTE**: To get a map only of SF, you need to create your own * BoundingBox * which can be done in the following way:\n",
    "```\n",
    "bbox = BoundingBox(north=max_lat, west=min_lon, south=min_lat, east=max_lon)\n",
    "geoplotlib.set_bbox(bbox)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1C: Linear Regression\n",
    "\n",
    "Full details, see Week 4's excercises.\n",
    "\n",
    "Start by picking an area and a crime type, I recommend starting with *all of SF* and `LARCENY/THEFT` so you don't have too many zero counts. Similarly, I recommend using 2015 data for training (big feel free grab everything if you feel a need for big data in your life).\n",
    "\n",
    "We will only make predictions for the interval 5am-5pm where the crime count for `LARCENY/THEFT` is more or less linearly increasing.\n",
    "\n",
    "We start by creating a scatterplot. \n",
    "- Put time-of-day (in the range 05:00-17:00) on the $x$-axis. \n",
    "- Next you go over the training data and bin and observed number of `LARCENY/THEFT` crimes per hour. You should have 12 data-points per day of training data.\n",
    "- Plot the (time-of-day, bin-count) tuples. Does it look like there is a correlation? \n",
    "- Report the Pearson-correlation. \n",
    "\n",
    "Finally, fit a straight line to the data, add the straight line to your scatterplot (similar to fig 14-1 in the book). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assingment 1D: Multiple Regression\n",
    "\n",
    "Let's improve the model by incorporating the total amount of crime in the area we're considering. Remember to:\n",
    "\n",
    "* Firstly, instead of using total crime vs. the crime we're considering, remember to use differences between _average_ total crime and observed crime.\n",
    "* Secondly, remember to rescale your input variables.\n",
    " \n",
    "Explain in your own words the reasoning between the two bullets above.\n",
    "\n",
    "Finally, create and fit the data set using multiple regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1E: Simple evaluation of regression models\n",
    "\n",
    "\n",
    "In this assignment, compare the accuary of the simple linear regression with the multiple regression to see what we have gained by increasing the model complexity. \n",
    "\n",
    "We test on two separate weeks of data from 2016. We use the first 7 days (Week A) of January 2016 and first 7 days of June 2016 (Week B). \n",
    "\n",
    "We measure error for a time-bin simply as the absolute difference between predicted crime and observed crime. The error for a week is simply the average error over all time-bins during that week.\n",
    "* Report the performance of Simple & Multiple Regression for Weeks A & B (four results total).\n",
    "* Comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
